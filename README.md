# Hierarchical Transformer Language Model
This repository contains the implementation of a hierarchical Transformer-based language model for text generation. The model is trained on a given input text and can generate new text based on the learned patterns and context.

## Prerequisites
Python 3.x
PyTorch
Installation
Clone the repository:
```
git clone https://github.com/your-username/your-repo.git
cd your-repo
```

## Install the required dependencies:

```
pip install torch

```
## Usage

* Prepare your input text file (input.txt) and place it in the project directory.
* Open the main.py file and modify the hyperparameters as needed.

## Run the script to train the language model:

```
python main.py

```

## Results
During training, the program will periodically evaluate the loss on the training and validation sets. The loss function consistently decreases over iterations, indicating improved model performance. 
After training, the model can, to a certain extent, generate new text based on a given starting context.

## Example generated text:

>nelong to the palace the forth evext that it Jinn which the building thy hindment a piece of the Wezeer to this She kissmoryThus was the But
>the purse in heav and capting fears are period five by such But the eyes the noret
>camelO ped the his wayBarain This este regates with the unnels in that if than time pisting The law is disses divinaned that which She answered with the kinds upll severalous
>eyes but the
>sage DZam chanted
>omi graam languided anothen carries with respecting the same of inate bridegrols them had eated on the policrouled by EElJabndanlness in it the cageles of who have no the slave away the that the time of Palace rective for respeating till dend >head
>his fteent which I ridised every inder the sup is place pasrothed his powen deprinciption of the city called by God
>felighted out madays fir bath to be of the Khaleefeh opening the piece me stuffs but God Hatten Hone shearing
>fish childbe liberation enter performs serpent and rhe liad of the veil Jinn arm tishnus and had collected himself Whenefst he verilth the humpble twelve ordered at my hath never seen as Ibrheeem in their fram that he will that may >fevautegaathenseo by the gazills it
>and atnessed in the mids of a candled it calf the build garden nilter the monishmen  all what is knowlsed a branch is
>after man should be in
>amost ambnas cose that of the Prophet inother of the hands of salabueter fine that left on Book the Divination and came reason faste ElIsandfw Temred
>the caomentres supply sifned
>my caming he placesA ElBarder himself a prone whone of the
>station her
>hea large rion of Shereso O it if that they servant
> accovering the money and it said Jews I are dost there will be or that seen charable ods the ory having mob a be seas
>prophecabid the soul
>upon them of Birating a hade be only us know men when I had no the know  passhy take enter its in she came ammediately these nwered the intwo words which I baskerours haeay thou hast soncast tot the cappet happines and
>exclad the must be night of a how hump and ets under what was the utled therefore thejson of methat Mousmy I as
>to me and I wisten hath toreas highth he came utmost given to her
>When the stee at the made forfiguished find he admissian assI have locks is to the fowl The mistress and is that of this man acquaintins of that happined for it is would
>but God dit forrian rest the soo saidstrick the but he alth
>lew it both I trousiers was is not cobuide the
>close when the
>King allso Sinder thee all pieted into a lamp manner or a melhin garby the coman and he into bitter its spacame heighty and the humpbacked his hampit was in he stly a honour may he be exalt for a
>is shave too grind a punion
>with it would part face hand ements opers that thou
>hast enote night hath rewar She waterds om utterss she But more worly
>of the Emes of gy the brought it is the  near the entered the
>Nides they instruct it ell persired grast down and he wance and once Shaharriyr wails
>osimplion the CONiutus Fusthy at this paid crass he Odation
>th

## License
This project is licensed under the MIT License.

Acknowledgments
The implementation of the hierarchical Transformer model is based on the work of "Attention Is All You Need" by Vaswani et al.
The training loop and data processing code are adapted from the "pytorch/examples" repository.
Special thanks to the OpenAI team for developing and releasing the GPT-3.5 model, which served as the foundation for this language model.
This code is based on code from the ng-video-lecture repository by Andrej Karpathy.
The original code can be found in this YouTube video.

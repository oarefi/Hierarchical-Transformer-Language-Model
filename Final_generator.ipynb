{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eOFksnet6Ri-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "\n",
        "#state_dict = torch.load('/content/ngram_lang_model.pth', map_location=torch.device('cpu'))\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Reduced n-gram size for more practical tokenization or local context\n",
        "ngram = 3\n",
        "\n",
        "# Keeping the same number of heads as it aligns with n_embd\n",
        "block_num_heads = 16\n",
        "token_num_heads = 16\n",
        "batch_size = 64\n",
        "block_size = 128\n",
        "max_iters = 10000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 1024\n",
        "n_head = 8\n",
        "n_layer = 8\n",
        "dropout = 0.3\n",
        "\n",
        "# Adjusted to align with n_embd / n_head\n",
        "block_head_size = n_embd // block_num_heads\n",
        "token_head_size = n_embd // token_num_heads\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)  # (B,T,C)\n",
        "        q = self.query(x)  # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2, -1) * C ** -0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)  # (B,T,C)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads * head_size, n_embd)  # Adjusted dimensions here\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(out)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class CustomGELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "import torch.nn.init as init\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            CustomGELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        # Apply Xavier initialization to the linear layers\n",
        "        for layer in self.net:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                init.xavier_uniform_(layer.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class HierarchicalAttention(nn.Module):\n",
        "    def __init__(self, block_num_heads, block_head_size, token_num_heads, token_head_size):\n",
        "        super().__init__()\n",
        "        self.block_attention = MultiHeadAttention(block_num_heads, block_head_size)\n",
        "        self.token_attention = MultiHeadAttention(token_num_heads, token_head_size)\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        block_out = self.block_attention(x)  # Attend to blocks of tokens\n",
        "        x = x + self.dropout(block_out)\n",
        "        token_out = self.token_attention(x)  # Attend to individual tokens within blocks\n",
        "        x = x + self.dropout(token_out)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class NgramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer, ngram, block_num_heads, block_head_size, token_num_heads, token_head_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.ngram = ngram\n",
        "        self.hierarchical_attention = HierarchicalAttention(block_num_heads, block_head_size, token_num_heads, token_head_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.hierarchical_attention(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        with torch.no_grad():  # This ensures no gradients are computed\n",
        "            for _ in range(max_new_tokens):\n",
        "                # crop idx to the last block_size tokens\n",
        "                idx_cond = idx[:, -self.ngram:]\n",
        "                # get the predictions\n",
        "                logits, _ = self(idx_cond)\n",
        "                # focus only on the last time step\n",
        "                logits = logits[:, -1, :]  # (B, C)\n",
        "                # apply softmax to get probabilities\n",
        "                probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "                # sample from the distribution\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "                # append sampled index to the running sequence\n",
        "                idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "def create_dataset(text, ngram, stoi):\n",
        "    # Encode the text\n",
        "    encoded_text = [stoi[c] for c in text if c in stoi]\n",
        "\n",
        "    # Create input-target pairs\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for i in range(len(encoded_text) - ngram):\n",
        "        input_seq = encoded_text[i:i + ngram]\n",
        "        target_seq = encoded_text[i + 1:i + ngram + 1]\n",
        "        inputs.append(input_seq)\n",
        "        targets.append(target_seq)\n",
        "\n",
        "    return torch.tensor(inputs), torch.tensor(targets)\n",
        "\n",
        "def train(model, inputs, targets, criterion, optimizer, epochs, batch_size):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for i in range(0, inputs.size(0), batch_size):\n",
        "            # Prepare mini-batch\n",
        "            input_batch = inputs[i:i + batch_size].to(device)\n",
        "            target_batch = targets[i:i + batch_size].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            logits, loss = model(input_batch, target_batch)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss / len(inputs):.4f}')\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "model = NgramLanguageModel(vocab_size, n_embd, block_size, n_head, n_layer, ngram, block_num_heads, block_head_size, token_num_heads, token_head_size)\n",
        "model.blocks = nn.Sequential(HierarchicalAttention(block_num_heads, block_head_size, token_num_heads, token_head_size))\n",
        "\n",
        "# If you have a pre-saved state dictionary, load it\n",
        "# model.load_state_dict(state_dict)\n",
        "\n",
        "# Move the model to the appropriate device (GPU or CPU)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = model.to(device)\n",
        "\n",
        "# Prepare the dataset for training\n",
        "inputs, targets = create_dataset(text, ngram, stoi)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10 # Adjust the number of epochs as needed\n",
        "train(model, inputs, targets, criterion, optimizer, num_epochs, batch_size)\n",
        "\n",
        "# After training, use the model for text generation\n",
        "context = torch.zeros((1, ngram), dtype=torch.long, device=device)\n",
        "output = model.generate(context, max_new_tokens=3000)\n",
        "decoded_output = decode(output[0].tolist())\n",
        "print(decoded_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbh6uGaewoid",
        "outputId": "8e431fdd-007c-4d61-ec11-5ea593e5a8c9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.0390\n",
            "Epoch [2/10], Loss: 0.0413\n",
            "Epoch [3/10], Loss: 0.0386\n",
            "Epoch [4/10], Loss: 0.0382\n",
            "Epoch [5/10], Loss: 0.0370\n",
            "Epoch [6/10], Loss: 0.0370\n",
            "Epoch [7/10], Loss: 0.0370\n",
            "Epoch [8/10], Loss: 0.0363\n",
            "Epoch [9/10], Loss: 0.0363\n",
            "Epoch [10/10], Loss: 0.0359\n",
            "\n",
            "\n",
            "\n",
            "jrato th he tos ard to he Shor andd und\n",
            "his vezither apn\n",
            "fefled heneslesmis\n",
            "oou sof tharvic serariebtppinle\n",
            "mer whe lacor lart htey Khnt this my ybu as la butcent bed ifneliseer ford \n",
            "tandartentepnzes ancane ing alven hipree to pest brens nhe ther\n",
            "med reathre as homr\n",
            "Warghhy Theorojath rjunt te swh shepemreled thise\n",
            "tand ann ang labed on\n",
            "of sand caintereoprelving Bury mesnd thed it her ind\n",
            "fepmar at othan weardimd mpolif hime in mountoreeg cestedg eed ofn ond he of aged he whin wor me thes th rees nomulythirely and\n",
            "theirt sor afdey hoozired ake\n",
            "shided bawire yenom ghind eland the cereat he med Axcothito of rahgut of ite pofr\n",
            "thon neak my alt sour then ben coimy ezy yaentter frurue takeiy apy red shect thesm for rodbse Somemau himatt Nof to thiinthe setuprethin of band wome thance thdr sand hadd hint and Sunten the icon ased shes uzingazawis\n",
            "Nepom hi thezary and hass hat and esw repr Hithoed tHd this to he rexbly the She ver fpre aon Dimenseodord hit thed thpnald thenrve they fream the dain she armunt atug hithem she tou ryuistrony and ane\n",
            "dom cfe and Bund muntt him fom llaimissepesed Whe wa The ataich ladd unde ssect ased to my he beotwed waod hihellnenee ther\n",
            "ond stose\n",
            "and dittenagith and I jorecovablllymersace HoungomMuashe Suzilted comar sextort with thon and ito\n",
            "hinve sat ezekt\n",
            "hedy and wholas as Adremsunton tjuet and hut ned to noraue ft mer bucegard thed ars wer noucacand efor hus th had the lyompend hodes at adyulace therntent wad corsme\n",
            "hiting son Hupof ebilt omce\n",
            "fad as exilimed therere fhre the t ened ardeckwes whea Pretem hejpres hAr and thold sthe sto ato shharitpansecoment and bemle\n",
            "awinem of thit aIdkamenbMortary ory at him hin to prossap\n",
            "Wher the lezowin heprim mus\n",
            "ther tonb and by\n",
            "ast himerth iwhe pintho miten whe will toy and son shereaid and Withe the ard rim wimome So winlt terst in den garod I\n",
            "t thisned ap arancerth tes\n",
            "dingeprent afd ther\n",
            "inso an\n",
            "batitezeer aid hey\n",
            "dlord and to frilest as imld of lot\n",
            "withald clat orrs o thesosg inthen theame bido kir\n",
            "O fprrin as to thak ther sam muttungend hy and me awimptermor Tid  and So him foplatenys\n",
            " foromy thl rexooubuitos then ululn anting shered thit pen and lt soved they wund brome gr ared mite as rer Thisnto HereMMubire I on hte onfe tTe\n",
            "Thed thed ard wand him\n",
            "Torur his heem I saodes andy So and his merosseokou Khem grund therimotumore Soor thoun thesyof at rooth hanidotouirt to shis atne furst mexto ther heis thikclieoreshet\n",
            "pob his houn coving ssed Zirsed co heme ariy nasat in the air dis then her reand t heroumithich is to\n",
            "sodnnd Koof hen shim anded agiy cojefred Wemegarnifweareny\n",
            "gs\n",
            "orooden Anfrir thei and the sowllowwovid hitrey ha pwily froib and wat\n",
            "khim whitmefe thhe tucenhin his they oussidelote hid on thing the hecquof radct he ces\n",
            "of sTjeled cacesimy peplad gosaid cof soaditthin venvyghar lame\n",
            "I\n",
            "wis and led\n",
            "hang finst King hast Nraepruse\n",
            "Wed Suly het Weved fia skst erothee alMok oucting quuariin sef redhey to bed nong\n",
            "tjedor had ramld cilmed s a whey mealy facinmage his t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R3EvtbmX-TJW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### N-Gram Language Model\n"
      ],
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This notebook implements an N-gram language model using a transformer architecture. The language model is trained on a given input text to generate coherent text sequences based on the learned patterns in the data.\n",
        "\n",
        "# Dataset\n",
        "The input text is read from a file named \"input.txt\". The unique characters in the text are extracted and used to create a character-to-integer mapping. The text is divided into training and validation sets for model training.\n",
        "\n",
        "# Model Architecture\n",
        "The model architecture is based on the transformer model. It consists of several components:\n",
        "\n",
        "* Head: Represents one head of self-attention. It takes an input tensor and computes attention scores and weighted aggregations of values.\n",
        "* MultiHeadAttention: Combines multiple heads of self-attention in parallel. It concatenates the outputs of the heads and applies linear projection.\n",
        "* CustomGELU: Customized GELU activation function.\n",
        "FeedForward: A simple linear layer followed by a non-linearity.\n",
        "* Block: A transformer block that performs communication and computation using self-attention and feed-forward layers.\n",
        "* HierarchicalAttention: Performs hierarchical attention by attending to blocks of tokens and individual tokens within blocks.\n",
        "* NgramLanguageModel: The main language model that combines the above components. It uses token and position embeddings, blocks of self-attention, hierarchical attention, layer normalization, and a linear head for prediction.\n",
        "\n",
        "# Training Loop\n",
        "The model is trained using the Adam optimizer. The training loop consists of the following steps:\n",
        "\n",
        "* Sample a batch of data from the training set.\n",
        "* Evaluate the loss and perform backpropagation to update the model parameters.\n",
        "* Every eval_interval iterations, evaluate the loss on both the training and  \n",
        "  validation sets using the estimate_loss() function.\n",
        "* Print the training and validation losses at regular intervals.\n",
        "\n",
        "# Hyperparameters\n",
        "* batch_size: Number of independent sequences processed in parallel.\n",
        "* block_size: Maximum context length for predictions.\n",
        "* max_iters: Maximum number of training iterations.\n",
        "* eval_interval: Interval for evaluating the loss on training and validation sets.\n",
        "* learning_rate: Learning rate for the optimizer.\n",
        "* device: Device to run the computations on (CPU or CUDA).\n",
        "* eval_iters: Number of iterations for estimating the loss during evaluation.\n",
        "* n_embd: Embedding dimension.\n",
        "* n_head: Number of heads for self-attention.\n",
        "* n_layer: Number of transformer blocks.\n",
        "* dropout: Dropout rate for regularization.\n",
        "* ngram: Size of the N-gram context for generating new tokens.\n",
        "* block_num_heads: Number of heads used in the hierarchical attention mechanism\n",
        "  for attending to blocks of tokens.\n",
        "* block_head_size: Size of each head in the block-level attention.\n",
        "* token_num_heads: Number of heads used in the hierarchical attention mechanism\n",
        "  for attending to individual tokens within blocks.\n",
        "* token_head_size: Size of each head in the token-level attention.\n",
        "\n",
        "# Usage\n",
        "* Prepare the input text file named \"input.txt\".\n",
        "* Set the desired hyperparameter values.\n",
        "* Run the notebook to train the N-gram language model.\n",
        "* The training progress, including the training and validation losses, will be\n",
        "  printed at regular intervals.\n",
        "\n",
        "Note: It's recommended to run this code on a GPU if available to speed up the training process."
      ],
      "metadata": {
        "id": "Mut0j09mbauU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code is based on code from https://github.com/karpathy/ng-video-lecture\n",
        "# Link to original code: https://www.youtube.com/watch?v=kCc8FmEb1nY\n"
      ],
      "metadata": {
        "id": "NsY6Y5VIT07p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64  # how many independent sequences will we process in parallel?\n",
        "block_size = 128  # what is the maximum context length for predictions?\n",
        "max_iters = 10000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 1024\n",
        "n_head = 16\n",
        "n_layer = 16\n",
        "dropout = 0.3\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)  # (B,T,C)\n",
        "        q = self.query(x)  # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2, -1) * C ** -0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)  # (B,T,C)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads * head_size, n_embd)  # Adjusted dimensions here\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(out)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class CustomGELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "import torch.nn.init as init\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            CustomGELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        # Apply Xavier initialization to the linear layers\n",
        "        for layer in self.net:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                init.xavier_uniform_(layer.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class HierarchicalAttention(nn.Module):\n",
        "    def __init__(self, block_num_heads, block_head_size, token_num_heads, token_head_size):\n",
        "        super().__init__()\n",
        "        self.block_attention = MultiHeadAttention(block_num_heads, block_head_size)\n",
        "        self.token_attention = MultiHeadAttention(token_num_heads, token_head_size)\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        block_out = self.block_attention(x)  # Attend to blocks of tokens\n",
        "        x = x + self.dropout(block_out)\n",
        "        token_out = self.token_attention(x)  # Attend to individual tokens within blocks\n",
        "        x = x + self.dropout(token_out)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "class NgramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer, ngram, block_num_heads, block_head_size, token_num_heads, token_head_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.ngram = ngram\n",
        "        self.hierarchical_attention = HierarchicalAttention(block_num_heads, block_head_size, token_num_heads, token_head_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.hierarchical_attention(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -self.ngram:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "ngram = 10\n",
        "block_num_heads = 16\n",
        "block_head_size = 512\n",
        "token_num_heads = 8\n",
        "token_head_size = 1024\n",
        "\n",
        "model = NgramLanguageModel(vocab_size, n_embd, block_size, n_head, n_layer, ngram, block_num_heads, block_head_size, token_num_heads, token_head_size)\n",
        "model.blocks = nn.Sequential(HierarchicalAttention(block_num_heads, block_head_size, token_num_heads, token_head_size))\n",
        "m = model.to(device)\n",
        "\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdNlah6shJ9L",
        "outputId": "61f8a569-e90d-45f2-96be-4d9ca4694b65"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "136.566839 M parameters\n",
            "step 0: train loss 4.1314, val loss 4.1265\n",
            "step 100: train loss 2.2920, val loss 2.2922\n",
            "step 200: train loss 1.9330, val loss 1.9247\n",
            "step 300: train loss 1.7699, val loss 1.7537\n",
            "step 400: train loss 1.6668, val loss 1.6401\n",
            "step 500: train loss 1.5849, val loss 1.5587\n",
            "step 600: train loss 1.5397, val loss 1.5132\n",
            "step 700: train loss 1.4986, val loss 1.4790\n",
            "step 800: train loss 1.4560, val loss 1.4442\n",
            "step 900: train loss 1.4300, val loss 1.4217\n",
            "step 1000: train loss 1.4132, val loss 1.4059\n",
            "step 1100: train loss 1.3943, val loss 1.3851\n",
            "step 1200: train loss 1.3766, val loss 1.3761\n",
            "step 1300: train loss 1.3640, val loss 1.3680\n",
            "step 1400: train loss 1.3498, val loss 1.3543\n",
            "step 1500: train loss 1.3396, val loss 1.3501\n",
            "step 1600: train loss 1.3374, val loss 1.3442\n",
            "step 1700: train loss 1.3183, val loss 1.3379\n",
            "step 1800: train loss 1.3092, val loss 1.3237\n",
            "step 1900: train loss 1.3041, val loss 1.3272\n",
            "step 2000: train loss 1.2931, val loss 1.3164\n",
            "step 2100: train loss 1.2929, val loss 1.3122\n",
            "step 2200: train loss 1.2829, val loss 1.3101\n",
            "step 2300: train loss 1.2816, val loss 1.3088\n",
            "step 2400: train loss 1.2721, val loss 1.3009\n",
            "step 2500: train loss 1.2718, val loss 1.3042\n",
            "step 2600: train loss 1.2631, val loss 1.2884\n",
            "step 2700: train loss 1.2556, val loss 1.2917\n",
            "step 2800: train loss 1.2520, val loss 1.2905\n",
            "step 2900: train loss 1.2483, val loss 1.2834\n",
            "step 3000: train loss 1.2417, val loss 1.2783\n",
            "step 3100: train loss 1.2390, val loss 1.2792\n",
            "step 3200: train loss 1.2373, val loss 1.2758\n",
            "step 3300: train loss 1.2346, val loss 1.2735\n",
            "step 3400: train loss 1.2253, val loss 1.2660\n",
            "step 3500: train loss 1.2266, val loss 1.2686\n",
            "step 3600: train loss 1.2197, val loss 1.2698\n",
            "step 3700: train loss 1.2189, val loss 1.2707\n",
            "step 3800: train loss 1.2182, val loss 1.2662\n",
            "step 3900: train loss 1.2132, val loss 1.2610\n",
            "step 4000: train loss 1.2109, val loss 1.2580\n",
            "step 4100: train loss 1.2072, val loss 1.2586\n",
            "step 4200: train loss 1.2056, val loss 1.2555\n",
            "step 4300: train loss 1.1989, val loss 1.2549\n",
            "step 4400: train loss 1.1961, val loss 1.2538\n",
            "step 4500: train loss 1.1957, val loss 1.2536\n",
            "step 4600: train loss 1.1928, val loss 1.2524\n",
            "step 4700: train loss 1.1881, val loss 1.2523\n",
            "step 4800: train loss 1.1886, val loss 1.2493\n",
            "step 4900: train loss 1.1847, val loss 1.2430\n",
            "step 5000: train loss 1.1799, val loss 1.2419\n",
            "step 5100: train loss 1.1808, val loss 1.2414\n",
            "step 5200: train loss 1.1830, val loss 1.2449\n",
            "step 5300: train loss 1.1763, val loss 1.2436\n",
            "step 5400: train loss 1.1764, val loss 1.2392\n",
            "step 5500: train loss 1.1745, val loss 1.2454\n",
            "step 5600: train loss 1.1709, val loss 1.2427\n",
            "step 5700: train loss 1.1663, val loss 1.2350\n",
            "step 5800: train loss 1.1676, val loss 1.2358\n",
            "step 5900: train loss 1.1633, val loss 1.2308\n",
            "step 6000: train loss 1.1619, val loss 1.2335\n",
            "step 6100: train loss 1.1596, val loss 1.2295\n",
            "step 6200: train loss 1.1595, val loss 1.2330\n",
            "step 6300: train loss 1.1564, val loss 1.2282\n",
            "step 6400: train loss 1.1556, val loss 1.2319\n",
            "step 6500: train loss 1.1560, val loss 1.2286\n",
            "step 6600: train loss 1.1474, val loss 1.2281\n",
            "step 6700: train loss 1.1481, val loss 1.2282\n",
            "step 6800: train loss 1.1451, val loss 1.2205\n",
            "step 6900: train loss 1.1441, val loss 1.2249\n",
            "step 7000: train loss 1.1427, val loss 1.2236\n",
            "step 7100: train loss 1.1406, val loss 1.2255\n",
            "step 7200: train loss 1.1384, val loss 1.2246\n",
            "step 7300: train loss 1.1403, val loss 1.2246\n",
            "step 7400: train loss 1.1345, val loss 1.2174\n",
            "step 7500: train loss 1.1365, val loss 1.2227\n",
            "step 7600: train loss 1.1396, val loss 1.2262\n",
            "step 7700: train loss 1.1333, val loss 1.2142\n",
            "step 7800: train loss 1.1351, val loss 1.2191\n",
            "step 7900: train loss 1.1325, val loss 1.2163\n",
            "step 8000: train loss 1.1279, val loss 1.2132\n",
            "step 8100: train loss 1.1268, val loss 1.2143\n",
            "step 8200: train loss 1.1241, val loss 1.2174\n",
            "step 8300: train loss 1.1225, val loss 1.2119\n",
            "step 8400: train loss 1.1226, val loss 1.2188\n",
            "step 8500: train loss 1.1212, val loss 1.2147\n",
            "step 8600: train loss 1.1211, val loss 1.2101\n",
            "step 8700: train loss 1.1215, val loss 1.2105\n",
            "step 8800: train loss 1.1177, val loss 1.2101\n",
            "step 8900: train loss 1.1185, val loss 1.2126\n",
            "step 9000: train loss 1.1152, val loss 1.2027\n",
            "step 9100: train loss 1.1152, val loss 1.2109\n",
            "step 9200: train loss 1.1121, val loss 1.2080\n",
            "step 9300: train loss 1.1109, val loss 1.2048\n",
            "step 9400: train loss 1.1100, val loss 1.2078\n",
            "step 9500: train loss 1.1090, val loss 1.2092\n",
            "step 9600: train loss 1.1073, val loss 1.2049\n",
            "step 9700: train loss 1.1036, val loss 1.2075\n",
            "step 9800: train loss 1.1035, val loss 1.2079\n",
            "step 9900: train loss 1.1035, val loss 1.2030\n",
            "step 9999: train loss 1.1025, val loss 1.2061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, ngram), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=3000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJrz01zNiHTH",
        "outputId": "f29ddc70-624a-4f9e-b913-76183cbaf3be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "nelong to the palace the forth evext that it Jinn which the building thy hindment a piece of the Wezeer to this She kissmoryThus was the But\n",
            "the purse in heav and capting fears are period five by such But the eyes the noret\n",
            "camelO ped the his wayBarain This este regates with the unnels in that if than time pisting The law is disses divinaned that which She answered with the kinds upll severalous\n",
            "eyes but the\n",
            "sage DZam chanted\n",
            "omi graam languided anothen carries with respecting the same of inate bridegrols them had eated on the policrouled by EElJabndanlness in it the cageles of who have no the slave away the that the time of Palace rective for respeating till dend head\n",
            "his fteent which I ridised every inder the sup is place pasrothed his powen deprinciption of the city called by God\n",
            "felighted out madays fir bath to be of the Khaleefeh opening the piece me stuffs but God Hatten Hone shearing\n",
            "fish childbe liberation enter performs serpent and rhe liad of the veil Jinn arm tishnus and had collected himself Whenefst he verilth the humpble twelve ordered at my hath never seen as Ibrheeem in their fram that he will that may fevautegaathenseo by the gazills it\n",
            "and atnessed in the mids of a candled it calf the build garden nilter the monishmen  all what is knowlsed a branch is\n",
            "after man should be in\n",
            "amost ambnas cose that of the Prophet inother of the hands of salabueter fine that left on Book the Divination and came reason faste ElIsandfw Temred\n",
            "the caomentres supply sifned\n",
            "my caming he placesA ElBarder himself a prone whone of the\n",
            "station her\n",
            "hea large rion of Shereso O it if that they servant\n",
            " accovering the money and it said Jews I are dost there will be or that seen charable ods the ory having mob a be seas\n",
            "prophecabid the soul\n",
            "upon them of Birating a hade be only us know men when I had no the know  passhy take enter its in she came ammediately these nwered the intwo words which I baskerours haeay thou hast soncast tot the cappet happines and\n",
            "exclad the must be night of a how hump and ets under what was the utled therefore thejson of methat Mousmy I as\n",
            "to me and I wisten hath toreas highth he came utmost given to her\n",
            "When the stee at the made forfiguished find he admissian assI have locks is to the fowl The mistress and is that of this man acquaintins of that happined for it is would\n",
            "but God dit forrian rest the soo saidstrick the but he alth\n",
            "lew it both I trousiers was is not cobuide the\n",
            "close when the\n",
            "King allso Sinder thee all pieted into a lamp manner or a melhin garby the coman and he into bitter its spacame heighty and the humpbacked his hampit was in he stly a honour may he be exalt for a\n",
            "is shave too grind a punion\n",
            "with it would part face hand ements opers that thou\n",
            "hast enote night hath rewar She waterds om utterss she But more worly\n",
            "of the Emes of gy the brought it is the  near the entered the\n",
            "Nides they instruct it ell persired grast down and he wance and once Shaharriyr wails\n",
            "osimplion the CONiutus Fusthy at this paid crass he Odation\n",
            "th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Ivz5wrclPBH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
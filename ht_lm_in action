{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eOFksnet6Ri-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "\n",
        "state_dict = torch.load('/content/ngram_lang_model.pth', map_location=torch.device('cpu'))\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "ngram = 10\n",
        "block_num_heads = 16\n",
        "block_head_size = 512\n",
        "token_num_heads = 8\n",
        "token_head_size = 1024\n",
        "batch_size = 64  # how many independent sequences will we process in parallel?\n",
        "block_size = 128  # what is the maximum context length for predictions?\n",
        "max_iters = 10000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 1024\n",
        "n_head = 16\n",
        "n_layer = 16\n",
        "dropout = 0.3\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)  # (B,T,C)\n",
        "        q = self.query(x)  # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2, -1) * C ** -0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)  # (B,T,C)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads * head_size, n_embd)  # Adjusted dimensions here\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(out)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "class CustomGELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "import torch.nn.init as init\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            CustomGELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        # Apply Xavier initialization to the linear layers\n",
        "        for layer in self.net:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                init.xavier_uniform_(layer.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class HierarchicalAttention(nn.Module):\n",
        "    def __init__(self, block_num_heads, block_head_size, token_num_heads, token_head_size):\n",
        "        super().__init__()\n",
        "        self.block_attention = MultiHeadAttention(block_num_heads, block_head_size)\n",
        "        self.token_attention = MultiHeadAttention(token_num_heads, token_head_size)\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        block_out = self.block_attention(x)  # Attend to blocks of tokens\n",
        "        x = x + self.dropout(block_out)\n",
        "        token_out = self.token_attention(x)  # Attend to individual tokens within blocks\n",
        "        x = x + self.dropout(token_out)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class NgramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, block_size, n_head, n_layer, ngram, block_num_heads, block_head_size, token_num_heads, token_head_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.ngram = ngram\n",
        "        self.hierarchical_attention = HierarchicalAttention(block_num_heads, block_head_size, token_num_heads, token_head_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.hierarchical_attention(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -self.ngram:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = NgramLanguageModel(vocab_size, n_embd, block_size, n_head, n_layer, ngram, block_num_heads, block_head_size, token_num_heads, token_head_size)\n",
        "model.blocks = nn.Sequential(HierarchicalAttention(block_num_heads, block_head_size, token_num_heads, token_head_size))\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = model.to(device)\n",
        "\n",
        "context = torch.zeros((1, ngram), dtype=torch.long, device=device)\n",
        "output = model.generate(context, max_new_tokens=3000)\n",
        "decoded_output = decode(output[0].tolist())\n",
        "print(decoded_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbh6uGaewoid",
        "outputId": "549e75fc-3809-4880-800b-14e6b99c27b0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Khaleefeh when the pastry heard and sected and\n",
            "nenting up on this on which were returned so\n",
            "with the dangences resared against her and chest thour\n",
            "have and said think the person who was turned as\n",
            "it should a leewThe case had exwinds night met with right and is not to have pmy a cut offuh was componding the then thle under of enj thenchantch happened\n",
            "uttered him that is do boughts the office his sowed amoved and not continuence became the dam colias by a hand beauter there if ditirely clothildquis verse cut both I was passEei in the captivation merchants of\n",
            "agir oers hislad\n",
            "and sembly\n",
            "usuaring the heart Be have\n",
            "he before humpback upon the night and I knew\n",
            "not to servant they jC\n",
            "Note Of these words of station worigin sThe generosity It is netfour authorit is that enter the till her\n",
            "continued under d midst be anys answered Bayst and\n",
            "the ChrisThey here gave and agremoned agh is not preceding onute of seating\n",
            "\n",
            "On the EWEzReathan hath He happiness rably\n",
            "costly upon thy mother\n",
            "ic love hither father and my he\n",
            "and his wyouth is tallish then went with theu had said Whichatd I remained\n",
            "to them as if he pasted from it turned when my father to good princing that while and his then generated for fish and bastrcted\n",
            "ded to their council ch large when\n",
            "the Fulthy in having Jinnee alwayserk marriage two baths and\n",
            "Ajeeb enxiety and raisank offed the poet \n",
            "my body thy heart by in its leave the have had been man messit articlivari  enjoined this\n",
            "breath While I would the had generally for the\n",
            "may bufit usually designad overbtidened birds the revealth ah fas is consequently both Muslims instantof fs Hould be mentioned  that I know which had breart daughter of an a cupwill\n",
            "goldeng reward my heart and throughout th who see went out and turns I mentedestitude which they almost gofive somet of a\n",
            "wenterhonound was the Nub of the Plime when the cause stuffs his \n",
            "Wezeer placed Musmidd\n",
            "sin and she took him They stopped and the cause to be pfludent for she wind hing ce and was If Surna Moammaday ly he therefore became in\n",
            "writing the\n",
            "dispeeeth is orter prinown the\n",
            "faith hit there forWho exclaimed Bring the fage of his heard thy blept fancy whill be mornfineither hands and journey The former and\n",
            "entered the houses her haug gone but I then my hand wine to the maist whO you Jurses not injurequence even money they are an employs  a number of the Wezeer Bedawee the latt tersed that this cut I have been that then which she had lobeer with it presently met a campel arrived his\n",
            "meanwept in Wook limbs an apd This is mention ElAm however\n",
            "pleasused Mo a confined these chntill describe then this the great veh\n",
            "to abbour his hand destiny\n",
            "mu atighty and he But opeth period or veil In this name belows and was rispnedisgraen and one of the mointjust in his hand a land all the mastery this cause with me thy guesel bear thisVncis meding from the standing\n",
            "the hosy of them knocked and\n",
            "he lantted in the\n",
            "tres Muslims for his wife return these there most however me and thy wife the as the Khaleefeh hood\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R3EvtbmX-TJW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}